# RAG Ingestion Patterns

## Pipeline Architecture

```
docs/
  ├── module-1/
  │   ├── chapter-1/
  │   │   ├── 01-lesson.md
  │   │   └── 02-lesson.md
  │   └── README.md
  └── ...

Pipeline:
Crawler → Parser → Chunker → Embedder → Uploader
```

## Document Crawler

```python
@dataclass
class DiscoveredFile:
    absolute_path: Path
    relative_path: str
    module: str
    chapter: int
    lesson: int | None
    is_readme: bool

class DocsCrawler:
    CHAPTER_PATTERN = re.compile(r"chapter-(\d+)-")
    LESSON_PATTERN = re.compile(r"^(\d+)-.*\.md$")

    def discover(self, docs_root: Path) -> Iterator[DiscoveredFile]:
        """Walk directory tree extracting metadata from paths."""
        for md_file in docs_root.rglob("*.md"):
            # Extract module from parent dirs
            # Extract chapter/lesson from path patterns
            yield DiscoveredFile(...)
```

## Frontmatter Parser with Hash

```python
import frontmatter
import hashlib

def compute_file_hash(file_path: str) -> str:
    """SHA-256 of file content for incremental updates."""
    with open(file_path, 'rb') as f:
        return hashlib.sha256(f.read()).hexdigest()

class LessonParser:
    DEFAULTS = {
        "proficiency_level": "A2",
        "hardware_tier": 1,
    }

    def parse(self, discovered: DiscoveredFile) -> LessonFile:
        file_hash = compute_file_hash(str(discovered.absolute_path))
        post = frontmatter.load(discovered.absolute_path)

        return LessonFile(
            file_path=str(discovered.absolute_path),
            relative_path=discovered.relative_path,
            file_hash=file_hash,
            title=post.get("title", discovered.filename),
            proficiency_level=post.get("proficiency_level", self.DEFAULTS["proficiency_level"]),
            hardware_tier=post.get("hardware_tier", self.DEFAULTS["hardware_tier"]),
            raw_content=post.content,
        )
```

## Semantic Chunker with Overlap

```python
class SectionChunker:
    """
    Production-grade chunking:
    - Split on ## headers (semantic boundaries)
    - Target 400 tokens per chunk (NVIDIA benchmark optimal)
    - 15% overlap for context continuity
    - Track prev/next relationships
    """
    SECTION_PATTERN = re.compile(r"(?=^## )", re.MULTILINE)
    TOKENS_PER_WORD = 1.3  # English text approximation

    def __init__(
        self,
        target_tokens: int = 400,
        max_tokens: int = 512,
        min_tokens: int = 100,
        overlap_percent: float = 0.15,
    ):
        self.target_words = int(target_tokens / self.TOKENS_PER_WORD)
        self.max_words = int(max_tokens / self.TOKENS_PER_WORD)
        self.min_words = int(min_tokens / self.TOKENS_PER_WORD)
        self.overlap_words = int(self.target_words * overlap_percent)

    def chunk(self, lesson: LessonFile) -> list[DocumentChunk]:
        # 1. Split by ## headers (semantic boundaries)
        sections = self._split_by_sections(lesson.raw_content)

        # 2. Merge small sections, split large ones
        normalized = self._normalize_section_sizes(sections)

        # 3. Create overlapping chunks with linked IDs
        chunks = []
        for idx, (title, text) in enumerate(normalized):
            content_hash = hashlib.sha256(text.encode()).hexdigest()[:16]
            chunk_id = f"{lesson.file_hash[:8]}_{content_hash}_{idx}"

            chunks.append(DocumentChunk(
                id=chunk_id,
                text=text,
                section_title=title,
                chunk_index=idx,
                total_chunks=len(normalized),
                prev_chunk_id=chunks[-1].id if chunks else None,
                next_chunk_id=None,  # Set after loop
                content_hash=content_hash,
                source_file_hash=lesson.file_hash,
                parent_doc_id=lesson.relative_path,
            ))

            # Link previous chunk to this one
            if len(chunks) > 1:
                chunks[-2].next_chunk_id = chunk_id

        return chunks

    def _split_by_sections(self, content: str) -> list[tuple[str, str]]:
        """Split on ## headers, return (title, content) pairs."""
        parts = self.SECTION_PATTERN.split(content)
        sections = []
        for part in parts:
            if part.strip():
                lines = part.split('\n', 1)
                title = lines[0].lstrip('#').strip() if lines[0].startswith('##') else None
                body = lines[1] if len(lines) > 1 else lines[0]
                sections.append((title, body.strip()))
        return sections

    def _normalize_section_sizes(self, sections: list[tuple]) -> list[tuple]:
        """Merge small sections, split large ones."""
        result = []
        buffer_title = None
        buffer_text = ""

        for title, text in sections:
            word_count = len(text.split())

            if word_count < self.min_words:
                # Merge with buffer
                buffer_text += f"\n\n## {title}\n{text}" if title else f"\n\n{text}"
                if not buffer_title and title:
                    buffer_title = title
            elif word_count > self.max_words:
                # Flush buffer first
                if buffer_text:
                    result.append((buffer_title, buffer_text.strip()))
                    buffer_title = None
                    buffer_text = ""
                # Split large section
                result.extend(self._split_large_section(title, text))
            else:
                # Flush buffer and add section
                if buffer_text:
                    result.append((buffer_title, buffer_text.strip()))
                    buffer_title = None
                    buffer_text = ""
                result.append((title, text))

        # Final flush
        if buffer_text:
            result.append((buffer_title, buffer_text.strip()))

        return result
```

## Batched Embeddings

```python
class OpenAIEmbedder:
    def __init__(
        self,
        model: str = "text-embedding-3-small",
        batch_size: int = 20,  # OpenAI recommendation
    ):
        self.client = OpenAI()
        self.model = model
        self.batch_size = batch_size
        self.dimensions = 1536  # text-embedding-3-small

    def embed_chunks(self, chunks: list[DocumentChunk]) -> list[EmbeddedChunk]:
        embedded = []

        for batch_start in range(0, len(chunks), self.batch_size):
            batch = chunks[batch_start:batch_start + self.batch_size]
            texts = [c.text for c in batch]

            response = self.client.embeddings.create(
                input=texts,
                model=self.model,
            )

            for chunk, emb_data in zip(batch, response.data):
                embedded.append(EmbeddedChunk(
                    **chunk.model_dump(),
                    embedding=emb_data.embedding,
                ))

        return embedded

    def embed_single(self, text: str) -> list[float]:
        """Embed single query text."""
        response = self.client.embeddings.create(
            input=[text],
            model=self.model,
        )
        return response.data[0].embedding
```

## Qdrant Uploader

```python
class QdrantUploader:
    def __init__(self, collection_name: str, batch_size: int = 100):
        self.client = QdrantClient(url=settings.qdrant_url, api_key=settings.qdrant_key)
        self.collection_name = collection_name
        self.batch_size = batch_size

    def ensure_collection(self, recreate: bool = False):
        """Create collection with proper vector config and indexes."""
        if recreate:
            self.client.delete_collection(self.collection_name)

        self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(
                size=1536,  # text-embedding-3-small
                distance=Distance.COSINE,
            ),
        )

        # Create payload indexes for efficient filtering
        indexes = [
            ("book_id", PayloadSchemaType.KEYWORD),
            ("module", PayloadSchemaType.KEYWORD),
            ("hardware_tier", PayloadSchemaType.INTEGER),
            ("chapter", PayloadSchemaType.INTEGER),
            ("lesson", PayloadSchemaType.INTEGER),
            ("proficiency_level", PayloadSchemaType.KEYWORD),
            ("parent_doc_id", PayloadSchemaType.KEYWORD),
            ("content_hash", PayloadSchemaType.KEYWORD),
            ("source_file_hash", PayloadSchemaType.KEYWORD),
        ]

        for field_name, field_type in indexes:
            self.client.create_payload_index(
                collection_name=self.collection_name,
                field_name=field_name,
                field_schema=field_type,
            )

    def upload(self, chunks: list[EmbeddedChunk]):
        """Batch upsert with full payload."""
        points = [
            PointStruct(
                id=chunk.id,
                vector=chunk.embedding,
                payload=chunk.to_payload(),
            )
            for chunk in chunks
        ]

        for i in range(0, len(points), self.batch_size):
            batch = points[i:i + self.batch_size]
            self.client.upsert(
                collection_name=self.collection_name,
                wait=True,
                points=batch,
            )

    def delete_by_file(self, source_file: str):
        """Delete all chunks from a specific file."""
        self.client.delete(
            collection_name=self.collection_name,
            points_selector=FilterSelector(
                filter=Filter(must=[
                    FieldCondition(key="source_file", match=MatchValue(value=source_file))
                ])
            ),
        )
```

## Qdrant-Native State Tracking

```python
class QdrantStateTracker:
    """Query Qdrant payloads directly for change detection. No external state DB needed."""

    def __init__(self, client: QdrantClient, collection_name: str, book_id: str):
        self.client = client
        self.collection_name = collection_name
        self.book_id = book_id

    def get_indexed_files(self) -> dict[str, IndexedFileInfo]:
        """Query Qdrant payloads to find what's indexed."""
        indexed = {}
        offset = None

        while True:
            points, next_offset = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=Filter(must=[
                    FieldCondition(key="book_id", match=MatchValue(value=self.book_id))
                ]),
                limit=100,
                offset=offset,
                with_payload=["source_file", "source_file_hash"],
                with_vectors=False,
            )

            for point in points:
                source_file = point.payload.get("source_file")
                if source_file not in indexed:
                    indexed[source_file] = IndexedFileInfo(
                        source_file=source_file,
                        file_hash=point.payload.get("source_file_hash"),
                        chunk_ids=[],
                    )
                indexed[source_file].chunk_ids.append(str(point.id))

            if next_offset is None:
                break
            offset = next_offset

        return indexed

    def detect_changes(self, current_files: dict[str, str]) -> ChangeSet:
        """Compare filesystem against Qdrant index."""
        indexed = self.get_indexed_files()
        indexed_paths = set(indexed.keys())
        current_paths = set(current_files.keys())

        return ChangeSet(
            new_files=[p for p in current_paths - indexed_paths],
            deleted_files=[p for p in indexed_paths - current_paths],
            modified_files=[
                p for p in current_paths & indexed_paths
                if current_files[p] != indexed[p].file_hash
            ],
        )
```

## Production Ingestion API

```python
@router.post("/trigger")
async def trigger_ingestion(
    request: TriggerRequest,
    background_tasks: BackgroundTasks,
):
    """Trigger ingestion job (incremental/full/recreate)."""
    job_id = create_job(request.mode, request.book_id)

    background_tasks.add_task(
        run_ingestion_job,
        job_id,
        request.mode,
        request.book_id,
        request.docs_path,
    )

    return {"job_id": job_id, "status": "queued"}

@router.get("/status/{job_id}")
async def get_job_status(job_id: int):
    """Check ingestion job progress."""
    return get_job(job_id)

@router.post("/webhook/github")
async def github_webhook(
    request: Request,
    background_tasks: BackgroundTasks,
):
    """Auto-trigger on GitHub push events."""
    # Verify signature
    # Trigger incremental ingestion
    pass
```

## Success Metrics

- **Ingestion speed**: ~500 chunks/minute with batching
- **Change detection accuracy**: 100% (content hash based)
- **Re-indexing time**: Only changed files processed
- **Storage efficiency**: No duplicate embeddings for unchanged content